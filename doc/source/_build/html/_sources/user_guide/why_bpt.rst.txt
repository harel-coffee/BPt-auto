.. _why_bpt:

{{ header }}

***********
Why BPt?
***********

BPt seeks to integrate a number of lower level packages towards providing a higher level package and user experience. 
The key point of interest here is that BPt is designed specifically for performing neuroimaging based machine learning.
Towards this end, it builds upon and offers augmented capabilities for core libraries pandas and scikit-learn.



- Scikit-learn is fundamentally a lower level package, one which provides a
  number of the building blocks used in our package.
  That said, scikit-learn does not impose any sort of inherent order on the
  analysis workflow, and is far more flexible than our library.
  Flexibility is highly useful for a number of use cases, but still
  leaves room for higher level packages.
  Specifically, higher level packages like ours can help to both impose
  a conceptual ordering of recommended steps,
  as well as abstracting away boilerplate code in addition to
  providing other convenience functions.
  While it is obviously possible for users to re-create all of
  the functionality found in our library
  with the base libraries we employ, it would require in some cases
  immense effort, thus the existence of our package. 

- We offer advanced and flexible hyper-parameter optimization via
  tight integration with the python nevergrad package.
  This allows BPt to offer a huge range of optimization strategies beyond the
  Random and Grid Searches found in scikit-learn. 
  Beyond offering differing optimization methods, Nevergrad importantly supports more advanced functionality such as
  nesting hyper-parameters with dictionaries or choice like objects.
  This functionality allows users to easily implement within 
  BPt almost auto-ML like powerful and expressive parameter searches.
 
- BPt allows hyper-parameter distributions to be associated directly with machine learning pipeline objects.
  This functionality we believe makes it easier to optimize hyper-parameters across 
  a number of pipeline pieces concurrently. We further allow preset hyper-parameter
  distributions to be selected or modified.
  This functionality allows less experienced users to still be able to perform hyper-parameter optimization 
  without specifying the sometimes complex parameters themselves.
  In the future we hope to support the sharing of user defined hyper-parameter 
  distributions (note: this feature is already supported in the multi-user version of the web interface).

- We introduce meta hyper-parameter objects, e.g., the :class:`Select<BPt.Select>` object, which allows specifying the choice between
  different pipeline objects as a hyper-parameter. For example, the choice between base two or more base models,
  say a Random Forest model and an Elastic Net Regression (each with their own associated distributions of hyper-parameters), 
  can be specified with the Select wrapper as itself a hyper-parameter.
  In this way, broader modelling strategies can be defined explicitly within the hyper-parameter search.
  This allows researchers the ability to easily perform properly nested model selection,
  and thus avoid common pitfalls associated with performing too many repeated internal validations.
  Along similar lines, we introduce functionality where features can themselves be set as binary hyper-parameters.
  This allows for the possibility of higher level optimizations.

- BPt introduced the concept of Scopes across all pipeline pieces.
  Scopes are a way for different pipeline pieces to optionally act on only a subset of
  features. While a similar functionality is provided via ColumnTransformers in scikit-learn,
  our package improves on this principle in a number of ways. In particular, a number of
  transformers and feature selection objects can alter in sometimes unpredictable ways the number
  of features (i.e., the number of features input before a PCA transformation, vs. the number of output features).
  Our package tracks these changes. What tracking these changes allows is for scopes to be set in downstream pipeline pieces,
  e.g., the model itself, and have that functional set of features passed to the piece still reflect the intention 
  of the original scope. For example, if one wanted to perform one hot encoding on a feature X,
  then further specify that a sub-model within an ensemble learner should only be provided feature X,
  our implemented scope system would make this possible. 
  Importantly, the number of output features from the one hot encoding does not need to be known ahead of time,
  which makes properly nesting transformations like this much more accessible.
  Scopes can be particularly useful in neuroimaging based ML where a
  user might have data from a number of different modalities,
  and further a combination of categorical and continuous variables,
  all of which they may want to dynamically treat differently. 

- We seek to more tightly couple in general the interaction between data loading, defining cross validation strategies,
  evaluating ML pipelines and making sense of results. For libraries like scikit-learn,
  this coupling is explicitly discouraged (which allows them to provide an extremely modular set of building blocks).
  On the other hand, by coupling these processes on our end, we can introduce a number of conveniences to the end-user.
  These span a number of common use cases associated with neuroimaging based ML,
  including: Allowing the previously introduced concept of Scope.
  Abstracting away a number of the considerations that must be made when
  dealing with loading, plotting and modelling across different data types (e.g., categorical vs. continuous).
  Abstracting away a number of the considerations that must be
  made when dealing with loading, plotting and modelling with missing data / NaN’s
  Data loading and visualization related functions, e.g., dropping outliers
  and automatically visually viewing the distributions of a number of input variables.
  The generation of automatic feature importances across different
  cross validation schemes, and their plotting.
  Exporting of loaded variable summary information to .docx format. 
  The ability to easily save and load full projects. And more!

- At the cost of some flexibility, but with the aims of reducing potentially redundant and cognitively demanding choices,
  the Model Pipeline’s constructed within BPt restrict the order in which the
  different pieces can be specified (e.g., Imputation must come before feature selection).
  That said, as all pipeline pieces are designed to accept custom objects,
  experienced users can easily get around this by passing in their own custom pipelines in relevant places.
  Whenever possible, we believe it to be a benefit to reduce researcher degrees of freedom.

- BPt provides a loader module which allows the arbitrary inclusion of non-typical 2D scikit-learn input directly integrated into the ML pipeline.
  This functionality is designed to work with the hyper-parameter optimization, scope and other features already mentioned,
  and allows the usage of common neuroimaging features such as 3D MRI volumes, time-series, connectomes, surface based inputs,
  and more. Explicitly, this functionality is designed to be used and evaluated in a properly nested machine learning context. 
  An immensely useful package in this regard is nilearn, and by extension nibabel.
  Nilearn provides a number of functions which work very nicely with this loader module.
  This can be seen as combining the functionality of our package and nilearn,
  as an alternative to combining scikit-learn and nilearn.
  While the latter is obviously possible and preferable for some users,
  we hope that providing a higher level interface is still useful to others. 

- Along with the loader module, we currently include a number of extension
  objects beyond those found in the base nilearn library.
  These span the extraction of Network metrics from an adjacency matrix, support for extracting regions of interest from static or time-series surfaces,
  the generation of randomly defined surface parcellations,and the automatic caching of user defined, potentially computationally expensive,
  loading transformations. We plan to continue to add more useful functions like these in the future.

- Additional measures of feature importance can be specified to be automatically computed. Further, by tracking how features change,
  it can be useful in certain circumstances to back project computed feature importances to their 
  original space (e.g., in the case of pipeline where surfaces from a few modalities are loaded from disk along
  with a number of phenotypic categorical variables, a parcellation applied on just the surface volumes,
  feature selection performed separately for say a number of different modalities, and then a base model evaluated, 
  feature importances from the base model can be automatically projected back to the different modalities surfaces).