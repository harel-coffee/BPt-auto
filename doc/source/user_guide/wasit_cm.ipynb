{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Waist Circumference with Diffusion Weighted Imaging\n",
    "\n",
    "This notebook using diffusion weighted imaging data, and subjects waist circumference in cm from the ABCD Study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import BPt as bp\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "ps = bp.ProblemSpec(n_jobs=8)\n",
    "ps.subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data needed\n",
    "\n",
    "Data is loaded from a large csv file with all of the features from release 2 of the ABCD study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_rds(names, eventname='baseline_year_1_arm_1'):\n",
    "    \n",
    "    data = pd.read_csv('data/nda_rds_201.csv',\n",
    "                       usecols=['src_subject_id', 'eventname'] + names,\n",
    "                       na_values=['777', 999, '999', 777])\n",
    "    \n",
    "    data = data.loc[data[data['eventname'] == eventname].index]\n",
    "    data = data.set_index('src_subject_id')\n",
    "    data = data.drop('eventname', axis=1)\n",
    "    \n",
    "    # Obsificate subject ID for public example\n",
    "    data.index = list(range(len(data)))\n",
    "    \n",
    "    # Return as pandas DataFrame cast to BPt Dataset\n",
    "    return bp.Dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way we can look at all column avaliable\n",
    "all_cols = list(pd.read_csv('data/nda_rds_201.csv', nrows=0))\n",
    "all_cols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable\n",
    "target_cols = ['anthro_waist_cm']\n",
    "\n",
    "# non input feature - i.e., those that inform \n",
    "non_input_cols = ['sex', 'rel_family_id']\n",
    "\n",
    "# We will use the fiber at dti measures\n",
    "dti_cols = [c for c in all_cols if '_fiber.at' in c and '.full.' in c]\n",
    "len(dti_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the helper function defined at the start to load these features in as a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_rds(target_cols + non_input_cols + dti_cols)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional, but will print out some extra verbosity when using the dataset operations\n",
    "data.verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we will do is tell the dataset what roles the different columns are. See: https://sahahn.github.io/BPt/user_guide/role.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_target(target_cols) # Note we doing data = data.func()\n",
    "data = data.set_non_input(non_input_cols)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note right off the bat.\n",
    "\n",
    "1. The verbosity printed us out two statements, about dropping rows. This is due to a constraint on columns of role 'non input' that there cannot be any NaN / missing data, so those lines just say 2 NaN's were found when loading the first non input column and 6 when loading the next.\n",
    "\n",
    "2. The values for sex are still 'F' and 'M', we will handle that next.\n",
    "\n",
    "3. Some columns with role data are missing values. We will handle that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We explicitly say this variable should be binary\n",
    "data.to_binary('sex', inplace=True)\n",
    "\n",
    "# We will ordinalize rel_family_id too\n",
    "data = data.ordinalize(scope='rel_family_id')\n",
    "\n",
    "data['non input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look into that NaN problem we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nan_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like most of the missing data is missing for everyone, i.e., if the above info founds columns with only a few missing values, we might want to do something different, but this tells us that when data is missing it is missing for all columns.\n",
    "\n",
    "We just drop any subjects with any NaN data below across the target variable and the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_nan_subjects(scope='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we need to worry about with data like this is corrupted data, i.e., data with values that don't make sense due to a failure in the automatic processing pipeline. Let's look at the target variable first, then the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot('target')\n",
    "data['target'].max(), data['target'].min(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah I don't know about that waist cm of 0 ...\n",
    "The below code can be used to try different values of outliers to drop, since it is not by default applied in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter_outliers_by_std(scope='target', n_std=5).plot('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 std seems okay, so let's actually apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter_outliers_by_std(scope='target', n_std=5, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of skew values for the dti data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'].skew().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks okay, let's choose the variable with the most extreme skew to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(scope='dmri_dti.full.fa_fiber.at_fmaj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we apply just a strict criteria of say 10 std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter_outliers_by_std(scope='data', n_std=10, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Test set. \n",
    "\n",
    "In this example project we are going to test a bunch of different Machine Learning Pipeline's. In order to avoid meta-issues of overfitting onto our dataset, we will therefore define a train-test split. The train set we will use to try different pipelines, then only with the best final pipeline will we use the test set. \n",
    "\n",
    "We will impose one extra constraint when applying the test split, namely that members of the same family, i.e., those with the same family id, stay in the same training or testing fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this to say we want to preserve families\n",
    "preserve_family = bp.CVStrategy(groups='rel_family_id')\n",
    "\n",
    "# Apply the test split\n",
    "data = data.set_test_split(size=.2,\n",
    "                           cv_strategy=preserve_family,\n",
    "                           random_state=6)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Different Pipelines\n",
    "\n",
    "First let's save some commonly used parameters in an object called the ProblemSpec, we will use all defaults except for the number of jobs, for that let's use n_jobs=8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = bp.ProblemSpec(n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we will use to evaluate different pipelines is bp.evaluate, let's start with an example with just a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = bp.Model('linear')\n",
    "\n",
    "results = bp.evaluate(pipeline=linear_model,\n",
    "                      dataset=data,\n",
    "                      problem_spec=ps,\n",
    "                      eval_verbose=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the verbose output that it has correctly detected a number of things including: a regression problem type, that we only want to use the training set and ... \n",
    "\n",
    "We get an instance of BPtEvaluator with the results. This stores all kinds of different information, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta weights\n",
    "results.get_fis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw predictions made from each fold\n",
    "results.get_preds_dfs()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All options are listed under: 'Saved Attributes' and 'Avaliable Methods'.\n",
    "\n",
    "Anyways, let's continue trying different models. We will use a ridge regression model. Let's also use the fact that the jupyter notebook is defining variables in global scope to clean up the evaluation code a bit so we don't have to keep copy and pasting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pipe(pipeline, **kwargs):\n",
    "    return bp.evaluate(pipeline=pipeline,\n",
    "                       dataset=data,\n",
    "                       problem_spec=ps,\n",
    "                       verbose=1,\n",
    "                       **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = bp.Model('ridge')\n",
    "\n",
    "# Add standard scaler before ridge model\n",
    "# We want this because the same amount of regularization is used across features\n",
    "ridge_pipe = bp.Pipeline([bp.Scaler('standard'), ridge_model])\n",
    "\n",
    "eval_pipe(ridge_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression has hyper-parameters though, what if just whatever the default value is, is not a good choice? We can add a parameter search to the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = bp.ParamSearch('RandomSearch', n_iter=64)\n",
    "\n",
    "ridge_search_model = bp.Model('ridge',\n",
    "                              params=1, # Referring to a preset distribution of hyper-params\n",
    "                              param_search=random_search)\n",
    "\n",
    "ridge_search_pipe = bp.Pipeline([bp.Scaler('standard'), ridge_search_model])\n",
    "    \n",
    "eval_pipe(ridge_search_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point we can also ask different questions, for example: What happens we evaluate the model on only one sex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male only model first\n",
    "eval_pipe(ridge_search_pipe,\n",
    "          subjects=bp.ValueSubset('sex', 'M', decode_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipe(ridge_search_pipe,\n",
    "          subjects=bp.ValueSubset('sex', 'F', decode_values=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see a decrease in performance for the male model, though it is a bit difficult to tell if that is just noise, or related to the smaller sample sizes. Atleast the results are close, which tells us that sex likely is not being exploited as a proxy! For example if we ran the two same sex only models and they did terrible, it would tell us that our original model had been just memorizing sex effects.\n",
    "\n",
    "Let's try a different choice of scaler next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_search_pipe = bp.Pipeline([bp.Scaler('robust'), ridge_search_model])\n",
    "eval_pipe(ridge_search_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_search_pipe = bp.Pipeline([bp.Scaler('quantile norm'), ridge_search_model])\n",
    "eval_pipe(ridge_search_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, thats a little bit of a boost, though notably the std between folds is higher. Let's keep it for now.\n",
    "\n",
    "What about different choices of hyper-parameter optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make the changes in place\n",
    "ridge_search_pipe.steps[1].param_search.search_type = 'TwoPointsDE'\n",
    "eval_pipe(ridge_search_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_search_pipe.steps[1].param_search.search_type = 'AdaptiveDiscreteOnePlusOne'\n",
    "eval_pipe(ridge_search_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so far we arn't really seeing sweeping differences when any of the parameters are changed, what about if we try some different complete models as well? Let's also start off by using some full default pipelines. We can see options with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt.default.pipelines import pipelines\n",
    "list(pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the elastic net first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt.default.pipelines import elastic_pipe\n",
    "elastic_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this object has imputation and also a one hot enocer for categorical variables built in. In our case those will just be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipe(elastic_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a light gbm model next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt.default.pipelines import lgbm_pipe\n",
    "eval_pipe(lgbm_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-linear svm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt.default.pipelines import svm_pipe\n",
    "eval_pipe(svm_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a linear svm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipe = bp.Pipeline([bp.Scaler('robust'),\n",
    "                        bp.Model('linear svm', params=1, param_search=random_search)])\n",
    "eval_pipe(sgd_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "\n",
    "We can also notably achieve a lot of the different modelling steps we just took, but in a much cleaner way, that is through special Compare input objects. For example let's run the comparisons between a few models. Compare basically does a grid search over all of the parameters, but in contrast to setting up the different options as a nested grid search, the full 5-fold CV is run for every combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of bp.Options as wrapped in bp.Compare\n",
    "compare_pipes = bp.Compare([bp.Option(sgd_pipe, name='sgd'),\n",
    "                            bp.Option(ridge_search_pipe, name='ridge'),\n",
    "                            bp.Option(elastic_pipe, name='elastic'),\n",
    "                            bp.Option(lgbm_pipe, name='lgbm')])\n",
    "\n",
    "# Pass as before as if a pipeline\n",
    "results = eval_pipe(compare_pipes)\n",
    "results.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('bpt': conda)",
   "language": "python",
   "name": "python39164bitbptconda7805b3f5d58e4b658b79cb94739371e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
